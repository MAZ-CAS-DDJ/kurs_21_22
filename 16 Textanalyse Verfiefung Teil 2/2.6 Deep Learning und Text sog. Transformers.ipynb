{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugginface and Transformers\n",
    "- Transformer sind keine Actionfiguren sondern eine Deeplearning Architektur \n",
    "- Hugginface ist eine open source firma die bestimmte fertige deep learning modelle zur Verfügung stellt\n",
    "- Wir müssen dazu deeplearning bibliotheken lokal installieren\n",
    "- Es gibt zwei wichtige: pytorch (Facebook) und tensorflow (Google)\n",
    "- more https://huggingface.co/transformers/task_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment detection jetzt erst recht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9989656209945679}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('I am really sad that things had to be so terrible with this lockdown.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9989978671073914}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('Sun is shining out of my ass.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering - ziemlich cool!\n",
    "sog. Extractive Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.513595461845398,\n",
       " 'start': 35,\n",
       " 'end': 59,\n",
       " 'answer': 'huggingface/transformers'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({'question': 'What is the name of the repository ?',\n",
    "                   'context': 'Pipeline have been included in the huggingface/transformers repository'\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9417340159416199, 'start': 66, 'end': 72, 'answer': 'Paris.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer({'question': 'Where has the tower been built?',\n",
    "                   'context': 'The Eiffel tower has been built in the 18th century and stands in Paris. '\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5862139463424683, 'start': 66, 'end': 72, 'answer': 'Paris.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer({'question': 'How hot is the sun?',\n",
    "                   'context': 'The Eiffel tower has been built in the 18th century and stands in Paris. '\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lückentext ausfüllen\n",
    "sog. Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927402257919312,\n",
      "  'sequence': '<s>HuggingFace is creating a tool that the community uses to '\n",
      "              'solve NLP tasks.</s>',\n",
      "  'token': 3944,\n",
      "  'token_str': 'Ġtool'},\n",
      " {'score': 0.11349397897720337,\n",
      "  'sequence': '<s>HuggingFace is creating a framework that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 7208,\n",
      "  'token_str': 'Ġframework'},\n",
      " {'score': 0.052435580641031265,\n",
      "  'sequence': '<s>HuggingFace is creating a library that the community uses to '\n",
      "              'solve NLP tasks.</s>',\n",
      "  'token': 5560,\n",
      "  'token_str': 'Ġlibrary'},\n",
      " {'score': 0.034935325384140015,\n",
      "  'sequence': '<s>HuggingFace is creating a database that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 8503,\n",
      "  'token_str': 'Ġdatabase'},\n",
      " {'score': 0.028602493926882744,\n",
      "  'sequence': '<s>HuggingFace is creating a prototype that the community uses '\n",
      "              'to solve NLP tasks.</s>',\n",
      "  'token': 17715,\n",
      "  'token_str': 'Ġprototype'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.15306979417800903,\n",
      "  'sequence': '<s>The man went skiing with his girlfriend not knowing that he '\n",
      "              'would have an accident.</s>',\n",
      "  'token': 6096,\n",
      "  'token_str': 'Ġgirlfriend'},\n",
      " {'score': 0.07086747139692307,\n",
      "  'sequence': '<s>The man went skiing with his daughter not knowing that he '\n",
      "              'would have an accident.</s>',\n",
      "  'token': 1354,\n",
      "  'token_str': 'Ġdaughter'},\n",
      " {'score': 0.06778453290462494,\n",
      "  'sequence': '<s>The man went skiing with his wife not knowing that he would '\n",
      "              'have an accident.</s>',\n",
      "  'token': 1141,\n",
      "  'token_str': 'Ġwife'},\n",
      " {'score': 0.06104987487196922,\n",
      "  'sequence': '<s>The man went skiing with his friends not knowing that he '\n",
      "              'would have an accident.</s>',\n",
      "  'token': 964,\n",
      "  'token_str': 'Ġfriends'},\n",
      " {'score': 0.0447111539542675,\n",
      "  'sequence': '<s>The man went skiing with his son not knowing that he would '\n",
      "              'have an accident.</s>',\n",
      "  'token': 979,\n",
      "  'token_str': 'Ġson'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(nlp(f\"The man went skiing with his {nlp.tokenizer.mask_token} not knowing that he would have an accident.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.032353002578020096,\n",
      "  'sequence': '<s>Its always the robots that repair things.</s>',\n",
      "  'token': 12129,\n",
      "  'token_str': 'Ġrobots'},\n",
      " {'score': 0.03077397681772709,\n",
      "  'sequence': '<s>Its always the ones that repair things.</s>',\n",
      "  'token': 1980,\n",
      "  'token_str': 'Ġones'},\n",
      " {'score': 0.028871040791273117,\n",
      "  'sequence': '<s>Its always the tools that repair things.</s>',\n",
      "  'token': 3270,\n",
      "  'token_str': 'Ġtools'},\n",
      " {'score': 0.02700759470462799,\n",
      "  'sequence': '<s>Its always the screws that repair things.</s>',\n",
      "  'token': 34242,\n",
      "  'token_str': 'Ġscrews'},\n",
      " {'score': 0.025950074195861816,\n",
      "  'sequence': '<s>Its always the humans that repair things.</s>',\n",
      "  'token': 5868,\n",
      "  'token_str': 'Ġhumans'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(nlp(f\"Its always the {nlp.tokenizer.mask_token} that repair things.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nächstes Wort erraten\n",
    "sog. Casual language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\", return_dict=True)\n",
    "sequence = f\"I could not believe that the answer is\"\n",
    "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "# get logits of last hidden state\n",
    "next_token_logits = model(input_ids).logits[:, -1, :]\n",
    "# filter\n",
    "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "# sample\n",
    "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "generated = torch.cat([input_ids, next_token], dim=-1)\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not believe that the answer is so\n"
     ]
    }
   ],
   "source": [
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ganzen text generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The corona lockdown was a full success.\\n\\n\"We\\'re still working on it,\" said Dr. David L. L. Litt, a professor of medicine at the University of California, San Francisco. \"We\\'re still working on it'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"The corona lockdown was a full success.\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text automatisch zusammenfassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "ARTICLE = \"\"\" Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "\n",
    "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n",
    "\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again.\n",
    "\n",
    "The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n",
    "\n",
    "Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled “ORANGE MARMALADE”, but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it.\n",
    "\n",
    "“Well!” thought Alice to herself, “after such a fall as this, I shall think nothing of tumbling down stairs! How brave they’ll all think me at home! Why, I wouldn’t say anything about it, even if I fell off the top of the house!” (Which was very likely true.)\n",
    "\n",
    "Down, down, down. Would the fall never come to an end? “I wonder how many miles I’ve fallen by this time?” she said aloud. “I must be getting somewhere near the centre of the earth. Let me see: that would be four thousand miles down, I think—” (for, you see, Alice had learnt several things of this sort in her lessons in the schoolroom, and though this was not a very good opportunity for showing off her knowledge, as there was no one to listen to her, still it was good practice to say it over) “—yes, that’s about the right distance—but then I wonder what Latitude or Longitude I’ve got to?” (Alice had no idea what Latitude was, or Longitude either, but thought they were nice grand words to say.)\n",
    "\n",
    "Presently she began again. “I wonder if I shall fall right through the earth! How funny it’ll seem to come out among the people that walk with their heads downward! The Antipathies, I think—” (she was rather glad there was no one listening, this time, as it didn’t sound at all the right word) “—but I shall have to ask them what the name of the country is, you know. Please, Ma’am, is this New Zealand or Australia?” (and she tried to curtsey as she spoke—fancy curtseying as you’re falling through the air! Do you think you could manage it?) “And what an ignorant little girl she’ll think me for asking! No, it’ll never do to ask: perhaps I shall see it written up somewhere.”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Alice was beginning to get tired of sitting by her sister on the bank, and of having nothing to do, when suddenly a White Rabbit with pink eyes ran close by her . She ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge . Alice had not a moment to think about stopping herself before she found herself falling down a very deep well .'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](huggin.jpeg \"How I felt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Und jetzt sogar noch in Deutsch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"Wenn er mit seinem Gilet daherkam und seiner Ledertasche, ein bisschen die Beine schleifen ließ, den Kopf ein wenig nach vorne gebeugt und dabei immer dieses Lachen im Gesicht, da hatte er etwas von einem älter gewordenen Jungen, der viel jünger wirkte, als er war. Und jetzt diese Nachricht: David Graeber ist gestorben, in Venedig. Er wurde 59 Jahre alt.      \n",
    "\n",
    "Es gibt ja nicht viele Linksradikale, die den Linksradikalismus nicht zur Clownerie verkommen lassen, sondern ernst meinen, und die zugleich zu globalen Superstars und Bestsellerautoren werden. \"Anarchist\" nannte er sich – oder wurde er genannt –, aber ob er wirklich einer war, das kann man diskutieren. Er war einfach der Meinung, dass Menschen ein solidarisches Miteinander pflegen und aufeinander achtgeben würden, wenn sie nicht in repressiven Strukturen eingepfercht wären, und er war überzeugt, \"dass Macht korrumpiert\". Anderen linken Strömungen oder gar Parteien fühlte er sich nicht richtig zugehörig, so war er vielleicht eher ein Anarchist mangels besserer Alternative.\n",
    "\n",
    "Graeber besaß noch etwas von dem Habitus früherer Revolutionäre und Intellektueller, die wichtigtuerische Aufgeblasenheit mancher akademischer Linker war nicht seine Sache, er war da viel bescheidener. Vielleicht hat das auch mit seiner Herkunft zu tun. Graeber wuchs in einer linken, jüdischen Arbeiterklassenfamilie auf, sein Vater kämpfte im Spanischen Bürgerkrieg.\n",
    "\n",
    "Weltberühmt und zu einer Figur der internationalen Linken machte den US-amerikanischen Anthropologen, der seit Jahren in London lehrte, die Finanzkrise vor zehn Jahren.\n",
    "\n",
    "Sein Buch Schulden. Die ersten 5.000 Jahre war ein Ereignis, natürlich auch deswegen, weil zockende Banken, weil Kredite, Budgetdefizite von Staaten das Thema der Stunde waren. Graeber zerlegte ein paar Mythen und blickte auf scheinbare Selbstverständlichkeiten mit einem neuen scharfen Blick. Sozialverhältnisse, die von Geld bestimmt werden, produzierten Gewalt, Entmenschlichung, Sklaverei, schrieb er. Zahlungsverhältnisse etablierten hierarchische Verhältnisse von Macht und Ohnmacht. Wo alle verschuldet seien, rennen viele nur mehr um das Überleben.\n",
    "\n",
    "Ursprungsmythen, wie sie die Ökonomie so liebt, wie das Märchen, dass frühere Gesellschaften einfach Gebrauchswerte tauschten, wischte Graeber vom Tisch: Solche Gesellschaften gab es nie. Immer schon nutzten Menschen Äquivalente zur Vereinfachung des Tausches. Geld aber, in Form von Banknoten und Münzen, war bis in die frühe Neuzeit sehr selten. Man brauchte nicht viel davon, wenn alle anschreiben ließen und höchstens zweimal im Jahr abrechneten.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'David Graeber ist gestorben, in Venedig. \"Anarchist\" nannte er sich – oder wurde er genannt –. Er war einfach der Meinung, dass Menschen ein solidarisches Miteinander pflegen und aufeinander achtgeben würden.'}]\n"
     ]
    }
   ],
   "source": [
    "print(ger_summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
